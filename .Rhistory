for(j in 1:r){
# create data
y = f + rnorm(n,mean=0,sd = .5)
# run validation set and 10-fold cv
vs[j] = which.min(valset_knn(x,y,1:50))
cv10[j] = which.min(do_10cv_knn(x,y,1:50))
x0 = (1:100)/101
# compute fit and error with k given by validation set
fit_vs = knnreg(as.matrix(x),as.matrix(y),k=vs[j])
pr_vs = predict(fit_vs,data.frame(x=as.matrix(x0)))
err_vs[j] = sum((pr_vs - f)^2)
# compute fit and error with k given by 10-cv
fit_cv10 = knnreg(as.matrix(x),as.matrix(y),k=cv10[j])
pr_cv10 = predict(fit_cv10,data.frame(x=as.matrix(x0)))
err_cv10[j] = sum((pr_cv10 - f)^2)
# compute fit and error with optimal k (determined earlier)
fit_opt = knnreg(as.matrix(x),as.matrix(y),k=15)
pr_opt = predict(fit_opt,data.frame(x=as.matrix(x0)))
err_opt[j] = sum((pr_opt - f)^2)
# print progress
print(j)
}
# the following are estimated mean squared errors
# note: those are not exactly numbers in lectures
# those numbers were generated with more repetitions
# in order to minimize effect of randomness
mean(err_cv10)
mean(err_vs)
mean(err_opt)
# plot results
par(mfrow = c(1,2))
hist(vs,main = 'Validation set approach', xlab = 'k', breaks = (0:14)*2.5)
lines(c(15,15),c(0,100), col= 'red',lty=2)
hist(cv10,main = '10-fold cross-validation', xlab = 'k', breaks = (0:14)*2.5)
lines(c(15,15),c(0,100), col= 'red',lty=2)
do_5cv_knn = function(X,Y,krange){
n = length(Y) # smaple size
# permute index set
permidx = sample(1:n,n)
X = as.matrix(X)[permidx,]
Y = Y[permidx]
# size of fold
foldsize = floor(n/10)
# for saving errors
valset_error = array(0,length(krange))
for(k in 1:length(krange)){
K = krange[k]
# the inner loop will be over the folds
for(j in 1:5){
# for fold 1-9 do same as last time
if(j < 5){
testidx = foldsize*(j-1) + (1:foldsize)
# take care with last fold, it might be larger
# reason: n/10 might not be an integer
}else{
testidx = (foldsize*(j-1)):n
}
fit = knnreg(as.matrix(X)[-testidx,,drop=FALSE],Y[-testidx],k=K)
#
pr = predict(fit, newdata = as.matrix(X)[testidx,,drop=FALSE])
valset_error[k] = valset_error[k] + mean((Y[testidx] - pr)^2)
} # end loop over folds
} # end loop over k
# the next line will output the result
return(valset_error)
}
source('~/Documents/University of Toronto/Courses/2018-2019 Fall/STA314/Lectures/Lecture2/10cv-1se-rule.R', echo=TRUE)
do_10cv_1se_knn = function(X,Y,krange){
n = length(Y) # smaple size
# permute index set
permidx = sample(1:n,n)
X = as.matrix(X)[permidx,]
Y = Y[permidx]
# size of fold
foldsize = floor(n/10)
# for saving errors
cv_error = array(0,length(krange))
# for saving standard errors
cv_se = array(0,length(krange))
for(k in 1:length(krange)){
K = krange[k]
# the inner loop will be over the folds
fold_err = array(0,10) # will contain error from each fold
# used for 1 se rule
for(j in 1:10){
# for fold 1-9 do same as last time
if(j < 10){
testidx = foldsize*(j-1) + (1:foldsize)
# take care with last fold, it might be larger
# reason: n/10 might not be an integer
}else{
testidx = (foldsize*(j-1)):n
}
fit = knnreg(as.matrix(X)[-testidx,,drop=FALSE],Y[-testidx],k=K)
pr = predict(fit, newdata = as.matrix(X)[testidx,,drop=FALSE])
fold_err[j] = mean((Y[testidx] - pr)^2)
} # end loop over folds
cv_error[k] = mean(fold_err)
cv_se[k] = sd(fold_err)
} # end loop over k
# create a list with two elements
res_list = list(cv_error,cv_se)
# give names to the elements in list
names(res_list) = c('cv_error','cv_se')
# the next line will outout the result
return(res_list)
}
re = do_10cv_1se_knn(x,y,1:50)
plot(re$cv_error)
for(k in 1:50){
lines(c(k,k),c(re$cv_error[k]-re$cv_se[k]*10^{-1/2},re$cv_error[k]+re$cv_se[k]*10^{-1/2}))
}
abline(a = min(re$cv_error) + re$cv_se[which.min(re$cv_error)]*10^(-1/2), b = 0, col='red')
mi.err = min(re$cv_error)
max((1:50)[which(re$cv_error < mi.err + re$cv_se*10^{-1/2})])
do_10cv_1se_knn = function(X,Y,krange){
n = length(Y) # smaple size
# permute index set
permidx = sample(1:n,n)
X = as.matrix(X)[permidx,]
Y = Y[permidx]
# size of fold
foldsize = floor(n/10)
# for saving errors
cv_error = array(0,length(krange))
# for saving standard errors
cv_se = array(0,length(krange))
for(k in 1:length(krange)){
K = krange[k]
# the inner loop will be over the folds
fold_err = array(0,10) # will contain error from each fold
# used for 1 se rule
for(j in 1:10){
# for fold 1-9 do same as last time
if(j < 10){
testidx = foldsize*(j-1) + (1:foldsize)
# take care with last fold, it might be larger
# reason: n/10 might not be an integer
}else{
testidx = (foldsize*(j-1)):n
}
fit = knnreg(as.matrix(X)[-testidx,,drop=FALSE],Y[-testidx],k=K)
pr = predict(fit, newdata = as.matrix(X)[testidx,,drop=FALSE])
fold_err[j] = mean((Y[testidx] - pr)^2)
} # end loop over folds
cv_error[k] = mean(fold_err)
cv_se[k] = sd(fold_err)
} # end loop over k
# create a list with two elements
res_list = list(cv_error,cv_se)
# give names to the elements in list
names(res_list) = c('cv_error','cv_se')
# the next line will outout the result
return(res_list)
}
re = do_10cv_1se_knn(x,y,1:50)
plot(re$cv_error)
for(k in 1:50){
lines(c(k,k),c(re$cv_error[k]-re$cv_se[k]*10^{-1/2},re$cv_error[k]+re$cv_se[k]*10^{-1/2}))
}
abline(a = min(re$cv_error) + re$cv_se[which.min(re$cv_error)]*10^(-1/2), b = 0, col='red')
mi.err = min(re$cv_error)
max((1:50)[which(re$cv_error < mi.err + re$cv_se*10^{-1/2})])
set.seed(42)
n = 100
x = runif(n, -1, 1)
eps = rnorm(n, mean = 0, sd = 0.25)
y1 = x + eps
y2 = 2 - (x+1)^2 + eps
y3 = 1 + sin(4*x) + eps
eps
clear
x=0
for(k in 1:3){ x = x+1}
x
x = 1 + (1:5)
x[3]
x
x = rnorm(10, mean = 0, sd = 1)
z = x[-3]
length(z)
set.seed(42)
rnorm(1,0,1)
set.seed(42)
1 + rnorm(1, 0, 1)
x = array(0, 3)
x
len(x)
len(x)
length(x)
for(i in 1:length(x)) {}
for(i in 1:length(x)) {x[i] = i^2}
x
for(i in 0:length(x)) {x[i] = i^2}
x
for(i in 1:length(x)) {x[i] = i^2}
x
rm(list = ls())
library(leaps) # this contains the function regsubsets
library(MASS)
library(ISLR)
Credit = read.csv('/Users/cassie/Desktop/STA314-Comp/trainingdata.csvCredit.csv')
Credit = read.csv('/Users/cassie/Desktop/STA314 R Work/Credit.csv')
fix(Credit)
Credit = Credit[,!(names(Credit) == 'X')]  # drop predictor 'X' from the data
# note: X is not a sensible predictor, it is the number of the data point
attach(Credit)
lm(Balance~.,data=Credit)
training = read.csv('/Users/cassie/Desktop/STA314-Comp/trainingdata.csv')
fix(training)
attach(training)
lm(y~.,data=training)
training = read.csv('/Users/cassie/Desktop/STA314-Comp/trainingdata.csv')
fix(training)
attach(training)
lm(y~.,data=training)
# look at forward stepwise
regfit.forward = regsubsets(y~.,data = training,method='forward')
# performs forward selection
summary(regfit.forward)
coef(regfit.forward,4)
coef(regfit.forward,8)
#-----------------------------------------------------------------------
# look at backward stepwise
regfit.backward = regsubsets(y~.,data = training,method='backward')
# performs backward selection
summary(regfit.backward)
coef(regfit.backward,4)
coef(regfit.backward,8)
# plot adjusted R^2, Cp, BIC against number of predictors
# use nvmax argument to set maximal number of variables considered
regfit.best = regsubsets(y~.,data = training,nvmax = 28)
# extract values for Cp, BIC, adjusted R^2 from the object regfit.best
adjr2 = summary(regfit.best)$adjr2
cp = summary(regfit.best)$cp
bic = summary(regfit.best)$bic
#selecting the models and compute
#want a bigger adjusted r^2, and smaller values of cp and bic
which.max(adjr2)
which.min(cp)
which.min(bic)
# plot adjusted R^2, Cp, BIC against number of predictors
# use nvmax argument to set maximal number of variables considered
regfit.best = regsubsets(y~.,data = training,nvmax = 8)
# extract values for Cp, BIC, adjusted R^2 from the object regfit.best
adjr2 = summary(regfit.best)$adjr2
cp = summary(regfit.best)$cp
bic = summary(regfit.best)$bic
#selecting the models and compute
#want a bigger adjusted r^2, and smaller values of cp and bic
which.max(adjr2)
which.min(cp)
which.min(bic)
#Submission - GOTTA ASK
da.sample = data.frame(cbind(1:500,  prlmMultiUseful))
rm(list = ls())
library(leaps) # this contains the function regsubsets
library(MASS)
library(ISLR)
Credit = read.csv('/Users/cassie/Desktop/STA314 R Work/Credit.csv')
fix(Credit)
Credit = Credit[,!(names(Credit) == 'X')]  # drop predictor 'X' from the data
# note: X is not a sensible predictor, it is the number of the data point
attach(Credit)
lm(Balance~.,data=Credit)
#-----------------------------------------------------------------------
# now look at different ways of selecting models
# first: best subset selection
regfit.best = regsubsets(Balance~.,data = Credit)
# performs exhaustive search
summary(regfit.best)
regfit.best = regsubsets(Balance~.,data = Credit,nvmax = 11)
summary(regfit.best)
# performs exhaustive search
summary(regfit.best)
regfit.best = regsubsets(Balance~.,data = Credit,nvmax = 11)
summary(regfit.best)
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,4)
# plot adjusted R^2, Cp, BIC against number of predictors
# use nvmax argument to set maximal number of variables considered
regfit.best = regsubsets(Balance~.,data = Credit,nvmax = 11)
# extract values for Cp, BIC, adjusted R^2 from the object regfit.best
adjr2 = summary(regfit.best)$adjr2
cp = summary(regfit.best)$cp
bic = summary(regfit.best)$bic
#selecting the models and compute
#want a bigger adjusted r^2, and smaller values of cp and bic
which.max(adjr2)
which.min(cp)
which.min(bic)
# plot the results
# red points will indictae the number of predictors selected by each
# Cp, BIC, Adjusted R^2
par(mfrow = c(1,3))
plot(adjr2,type='l',xlab='number of predictors',ylab = 'Adjusted R^2')
points(adjr2)
points(which.max(adjr2),adjr2[which.max(adjr2)],col = 'red')
plot(cp,type='l',xlab='number of predictors',ylab = 'Cp')
points(cp)
points(which.min(cp),cp[which.min(cp)],col = 'red')
plot(bic,type='l',xlab='number of predictors',ylab = 'BIC')
points(bic)
points(which.min(bic),bic[which.min(bic)],col = 'red')
# adjusted R^2 goes up
#
#---------------------------------------------------------------------------
# plot the results with smaller range of number predictors
# red points will indictae the number of predictors selected by each
# Cp, BIC, Adjusted R^2
par(mfrow = c(1,3))
plot(3:12,adjr2[3:12],type='l',xlab='number of predictors',ylab = 'Adjusted R^2')
points(3:12,adjr2[3:12])
points(which.max(adjr2),adjr2[which.max(adjr2)],col = 'red')
plot(3:12,cp[3:12],type='l',xlab='number of predictors',ylab = 'Cp')
points(3:12,cp[3:12])
points(which.min(cp),cp[which.min(cp)],col = 'red')
plot(3:12,bic[3:12],type='l',xlab='number of predictors',ylab = 'BIC')
points(3:12,bic[3:12])
points(which.min(bic),bic[which.min(bic)],col = 'red')
# clear workspace
rm(list = ls())
library(leaps) # this contains the function regsubsets
library(MASS)
library(ISLR)
training = read.csv('/Users/cassie/Desktop/STA314-Comp/trainingdata.csv')
fix(training)
attach(training)
fix(training)
attach(training)
lm(y~.,data=training)
#-----------------------------------------------------------------------
# now look at different ways of selecting models
#Best subset selection WAS NOT RECOMMENDED
# first: best subset selection
regfit.best = regsubsets(y~.,data = training)
# performs exhaustive search
summary(regfit.best)
regfit.best = regsubsets(y~.,data = training,nvmax = 11)
summary(regfit.best)
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,4)
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,5)
regfit.best = regsubsets(y~.,data = training,nvmax = 11)
summary(regfit.best)
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,5)
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,4)
#-----------------------------------------------------------------------
# look at forward stepwise
regfit.forward = regsubsets(y~.,data = training,method='forward')
# to extract coefficients from one particular model,
# for example model with 4 predictors
coef(regfit.best,4)
#-----------------------------------------------------------------------
# look at forward stepwise
regfit.forward = regsubsets(y~.,data = training,method='forward')
# performs forward selection
summary(regfit.forward)
# note: for example for k = 4 we get a different model compared
# to the result from best subset selection.
coef(regfit.forward,4)
coef(regfit.forward,8)
#-----------------------------------------------------------------------
# look at backward stepwise
regfit.backward = regsubsets(y~.,data = training,method='backward')
# performs backward selection
summary(regfit.backward)
coef(regfit.backward,4)
coef(regfit.backward,8)
#X1, X12, X23, X25
coef(regfit.backward,5)
#-----------------------------------------------------------------------
# look at forward stepwise
regfit.forward = regsubsets(y~.,data = training,method='forward')
# performs forward selection
summary(regfit.forward)
# note: for example for k = 4 we get a different model compared
# to the result from best subset selection.
coef(regfit.forward,4)
coef(regfit.forward,5)
# clear workspace
rm(list = ls())
library(leaps) # this contains the function regsubsets
library(MASS)
library(ISLR)
training = read.csv('/Users/cassie/Desktop/STA314-Comp/trainingdata.csv')
fix(training)
attach(training)
lm(y~.,data=training)
# plot adjusted R^2, Cp, BIC against number of predictors
# use nvmax argument to set maximal number of variables considered
regfit.best = regsubsets(y~.,data = training,nvmax = 28)
# extract values for Cp, BIC, adjusted R^2 from the object regfit.best
adjr2 = summary(regfit.best)$adjr2
cp = summary(regfit.best)$cp
bic = summary(regfit.best)$bic
#selecting the models and compute
#want a bigger adjusted r^2, and smaller values of cp and bic
which.max(adjr2)
which.min(cp)
which.min(bic)
# clear workspace
rm(list = ls())
#load libraries
library(caret) #for k-nn regression
library(class) #for k-nn regression
# clear workspace
rm(list = ls())
#load libraries
library(caret) #for k-nn regression
library(class) #for k-nn regression
#getwd() # show current working directory. this is where on the computer
# files are saved by default and where R looks ofr files
setwd('/Users/cassie/Desktop/STA314-Comp/') # change to a different directory
# pick the right directory on your computer
mydataNew = read.csv(file = 'trainingdata.csv') # read a file from the working directory
fix(mydata) # take a look at data in table form
# pick the right directory on your computer
mydataNew = read.csv(file = 'trainingdata.csv') # read a file from the working directory
fix(mydataNew) # take a look at data in table form
names(mydataNew) # names of entries in res
attach(mydataNew) # make elements of res available in workspace
y = mydata$y # values of response variable
y = mydata$y # values of response variable
#General MultiLinear Regression
lmMulti = lm(y ~ ., data = mydataNew)
lmMulti$coefficients
summary(lmMulti)
prlmMulti = predict(lmMulti)
#Removed predictors that have a p-value greater than 0.05
lmMultiUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25, data = mydataNew)
summary(lmMultiUseful)
prlmMultiUseful = predict(lmMultiUseful)
plot(prlmMultiUseful, y - prlmMultiUseful,ylim = c(-10,10), xlab = 'predicted values', ylab = 'residuals', main = 'Useful Predictors from Multi')
length(prlmMultiUseful)
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
prlmMultiInteractionUseful = predict(lmMultiInteractionUseful)
#General MultiLinear Regression
lmMulti = lm(y ~ ., data = mydataNew)
lmMulti$coefficients
summary(lmMulti)
prlmMulti = predict(lmMulti)
#Removed predictors that have a p-value greater than 0.05
lmMultiUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25, data = mydataNew)
summary(lmMultiUseful)
prlmMultiUseful = predict(lmMultiUseful)
plot(prlmMultiUseful, y - prlmMultiUseful,ylim = c(-10,10), xlab = 'predicted values', ylab = 'residuals', main = 'Useful Predictors from Multi')
length(prlmMultiUseful)
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
#Removed predictors that have a p-value greater than 0.05
lmMultiUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25, data = mydataNew)
summary(lmMultiUseful)
prlmMultiUseful = predict(lmMultiUseful)
plot(prlmMultiUseful, y - prlmMultiUseful,ylim = c(-100,100), xlab = 'predicted values', ylab = 'residuals', main = 'Useful Predictors from Multi')
length(prlmMultiUseful)
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
lmMultiInteractionUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25+(X1:X25)+(X2:X8)+(X2:X12)+(X3:X8)+(X3:X12)+(X4:X8)+(X8:X25)+(X12:X13), data = mydataNew)
lmMultiInteractionUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25+(X1:X25)+(X2:X8)+(X2:X12)+(X3:X8)+(X3:X12)+(X4:X8)+(X8:X25)+(X12:X13), data = mydataNew)
summary(lmMultiInteractionUseful)
prlmMultiInteractionUseful = predict(lmMultiInteractionUseful)
plot(prlmMultiInteractionUseful, y - prlmMultiInteractionUseful,ylim = c(-50,50), xlim = c(-50, 50), xlab = 'predicted values', ylab = 'residuals', main = 'Multi Interaction with Useful Predictors')
lmMultiInteractionUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25+(X1:X25)+(X2:X8)+(X2:X12)+(X3:X8)+(X3:X12)+(X4:X8)+(X8:X25)+(X12:X13), data = mydataNew)
summary(lmMultiInteractionUseful)
prlmMultiInteractionUseful = predict(lmMultiInteractionUseful)
plot(prlmMultiInteractionUseful, y - prlmMultiInteractionUseful,ylim = c(-50,50), xlab = 'predicted values', ylab = 'residuals', main = 'Multi Interaction with Useful Predictors')
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
lmMultiInteractionUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25+(X1:X25)+(X2:X8)+(X2:X12)+(X3:X8)+(X3:X12)+(X4:X8)+(X8:X25)+(X12:X13), data = mydataNew)
summary(lmMultiInteractionUseful)
prlmMultiInteractionUseful = predict(lmMultiInteractionUseful)
plot(prlmMultiInteractionUseful, y - prlmMultiInteractionUseful,ylim = c(-5,5), xlab = 'predicted values', ylab = 'residuals', main = 'Multi Interaction with Useful Predictors')
#General MultiLinear Regression
lmMulti = lm(y ~ ., data = mydataNew)
lmMulti$coefficients
summary(lmMulti)
prlmMulti = predict(lmMulti)
#Removed predictors that have a p-value greater than 0.05
lmMultiUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25, data = mydataNew)
summary(lmMultiUseful)
prlmMultiUseful = predict(lmMultiUseful)
plot(prlmMultiUseful, y - prlmMultiUseful,ylim = c(-5,5), xlab = 'predicted values', ylab = 'residuals', main = 'Useful Predictors from Multi')
length(prlmMultiUseful)
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
lmMultiInteractionUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25+(X1:X25)+(X2:X8)+(X2:X12)+(X3:X8)+(X3:X12)+(X4:X8)+(X8:X25)+(X12:X13), data = mydataNew)
summary(lmMultiInteractionUseful)
prlmMultiInteractionUseful = predict(lmMultiInteractionUseful)
plot(prlmMultiInteractionUseful, y - prlmMultiInteractionUseful,ylim = c(-5,5), xlab = 'predicted values', ylab = 'residuals', main = 'Multi Interaction with Useful Predictors')
#Removed predictors that have a p-value greater than 0.05
lmMultiUseful = lm(y ~ X1+X2+X3+X4+X8+X12+X13+X23+X24+X25, data = mydataNew)
summary(lmMultiUseful)
#MultiLinear Regression with Interaction
lmMultiInteraction = lm(y ~ (X1+X2+X3+X4+X8+X12+X13+X23+X24+X25)^2, data = mydataNew)
summary(lmMultiInteraction)
lmMulti$coefficients
summary(lmMulti)
